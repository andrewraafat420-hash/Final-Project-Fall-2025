from flask import Flask, render_template, request, jsonify
import tensorflow as tf
from tensorflow.keras.preprocessing import image
import numpy as np
from PIL import Image
import io

app = Flask(__name__)

# --- 1. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª (Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ø£Ø¨Ø¬Ø¯ÙŠ Ø§Ù„ØµØ­ÙŠØ­) ---
MODELS_CONFIG = {
    'cte': {
        'path': 'CTE_analyzer_model 2.h5', 
        'classes': {0: 'Stroke (Ø¬Ù„Ø·Ø©)', 1: 'Normal (Ø³Ù„ÙŠÙ…)'},
        'target_size': (224, 224),
        'needs_rescaling': False, 
        'model': None
    },
    'malaria': {
        'path': 'Malaria_analyzer_model.h5', 
        'classes': {0: 'Parasitized (Ù…ØµØ§Ø¨)', 1: 'Uninfected (Ø³Ù„ÙŠÙ…)'},
        'target_size': (180, 180),
        'needs_rescaling': False, # Ø¹Ø·Ù„Ù†Ø§Ù‡Ø§ Ù„Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø«Ø¨Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠ
        'model': None
    }
}

print("â³ Loading Models into memory... Please wait.")
for key, config in MODELS_CONFIG.items():
    try:
        config['model'] = tf.keras.models.load_model(config['path'])
        print(f"âœ… {key.upper()} Model Loaded!")
    except Exception as e:
        print(f"âŒ Error loading {key} model: {e}")

# --- 2. Ø¯Ø§Ù„Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± ---
def preprocess_image(img_bytes, config):
    img = Image.open(io.BytesIO(img_bytes))
    if img.mode != "RGB":
        img = img.convert("RGB")
    
    img = img.resize(config['target_size'])
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    
    if config['needs_rescaling']:
        img_array = img_array / 255.0
    
    return img_array

# --- 3. Ø§Ù„Ù€ Route Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„ØªÙˆÙ‚Ø¹ ---
@app.route('/')
def index():
    return render_template('index.html')

@app.route('/predict', methods=['POST'])
def predict():
    if 'file' not in request.files:
        return jsonify({'error': 'No file uploaded'})
    
    file = request.files['file']
    disease_type = request.form.get('disease_type')
    
    if file.filename == '' or disease_type not in MODELS_CONFIG:
        return jsonify({'error': 'Invalid request'})
    
    selected_config = MODELS_CONFIG[disease_type]
    
    if selected_config['model'] is None:
        return jsonify({'error': 'Model not ready'})

    try:
        img_bytes = file.read()
        processed_img = preprocess_image(img_bytes, selected_config)
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙˆÙ‚Ø¹ (Inference)
        raw_prediction = selected_config['model'].predict(processed_img)
        
        # ğŸ§ª "Ø§Ù„Ø³Ø± Ø§Ù„Ù‡Ù†Ø¯ÙŠ" Ù„Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ù€ 641% ÙˆØ§Ù„Ù€ 139%:
        # Ø¯Ø§Ù„Ø© Softmax Ø¨ØªØ¬Ø¨Ø± Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª ÙŠÙƒÙˆÙ† 1 (ÙŠØ¹Ù†ÙŠ 100%)
        probabilities = tf.nn.softmax(raw_prediction[0]).numpy()
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø£Ø¹Ù„Ù‰ Ø«Ù‚Ø©
        class_idx = int(np.argmax(probabilities))
        confidence = float(np.max(probabilities) * 100) # Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù…Ø³ØªØ­ÙŠÙ„ ØªØ¹Ø¯ÙŠ 100%
            
        result_text = selected_config['classes'].get(class_idx, "Unknown")
        
        # ØªØ­Ø¯ÙŠØ¯ Ù„ÙˆÙ† Ø§Ù„Ø­Ø§Ù„Ø© ÙÙŠ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©
        is_safe = any(word in result_text for word in ['Normal', 'Uninfected', 'Ø³Ù„ÙŠÙ…'])
        
        return jsonify({
            'result': result_text,
            'confidence': f"{confidence:.2f}%",
            'is_danger': 0 if is_safe else 1
        })
        
    except Exception as e:
        print(f"Prediction Error: {e}")
        return jsonify({'error': str(e)})

if __name__ == '__main__':
    app.run(debug=True, port=5000)

if __name__ == '__main__':
    app.run(debug=True)
    from waitress import serve
    serve(app, host='0.0.0.0', port=5000)
